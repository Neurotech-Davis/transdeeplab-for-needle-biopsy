[15:44:17.455] Namespace(accumulation_steps=None, amp_opt_level='O1', base_lr=0.05, batch_size=24, cache_mode='part', config_file='swin_224_7_4level', dataset='Synapse', deterministic=1, eval=False, img_size=224, list_dir='./lists/lists_Synapse', max_epochs=200, max_iterations=30000, n_gpu=1, num_classes=9, opts=None, output_dir='.', resume=None, root_path='./data/train_npz', seed=1234, tag=None, throughput=False, use_checkpoint=False, zip=False)
[15:44:17.465] 93 iterations per epoch. 18600 max iterations 
[15:45:33.674] Namespace(accumulation_steps=None, amp_opt_level='O1', base_lr=0.0125, batch_size=6, cache_mode='part', config_file='swin_224_7_4level', dataset='Synapse', deterministic=1, eval=False, img_size=224, list_dir='./lists/lists_Synapse', max_epochs=200, max_iterations=30000, n_gpu=1, num_classes=9, opts=None, output_dir='.', resume=None, root_path='./data/train_npz', seed=1234, tag=None, throughput=False, use_checkpoint=False, zip=False)
[15:45:33.686] 369 iterations per epoch. 73800 max iterations 
[15:45:37.783] iteration 1 : loss : 1.525760, loss_ce: 2.428683
[15:45:38.193] iteration 2 : loss : 1.476514, loss_ce: 2.262931
[15:45:38.581] iteration 3 : loss : 1.340740, loss_ce: 1.942032
[15:45:38.962] iteration 4 : loss : 1.184834, loss_ce: 1.608676
[15:45:39.358] iteration 5 : loss : 1.021038, loss_ce: 1.208565
[15:45:39.742] iteration 6 : loss : 0.892233, loss_ce: 0.926395
[15:45:40.128] iteration 7 : loss : 0.755062, loss_ce: 0.608195
[15:45:40.512] iteration 8 : loss : 0.655158, loss_ce: 0.334688
[15:45:40.900] iteration 9 : loss : 0.663067, loss_ce: 0.353845
[15:45:41.281] iteration 10 : loss : 0.645326, loss_ce: 0.294483
[15:45:41.660] iteration 11 : loss : 0.605528, loss_ce: 0.191494
[15:45:42.037] iteration 12 : loss : 0.698995, loss_ce: 0.415542
[15:45:42.426] iteration 13 : loss : 0.745252, loss_ce: 0.527750
[15:45:42.798] iteration 14 : loss : 0.616918, loss_ce: 0.209778
[15:45:43.178] iteration 15 : loss : 0.715627, loss_ce: 0.451684
[15:45:43.559] iteration 16 : loss : 0.683511, loss_ce: 0.373315
[15:45:43.946] iteration 17 : loss : 0.711030, loss_ce: 0.441008
[15:45:44.331] iteration 18 : loss : 0.605577, loss_ce: 0.180287
[15:45:44.729] iteration 19 : loss : 0.633118, loss_ce: 0.248310
[15:45:45.123] iteration 20 : loss : 0.623872, loss_ce: 0.225822
[15:45:46.342] iteration 21 : loss : 0.679843, loss_ce: 0.364013
[15:45:46.719] iteration 22 : loss : 0.664786, loss_ce: 0.326215
[15:45:47.099] iteration 23 : loss : 0.717971, loss_ce: 0.458705
[15:45:47.482] iteration 24 : loss : 0.841802, loss_ce: 0.764092
[15:45:47.871] iteration 25 : loss : 0.640617, loss_ce: 0.269869
[15:45:48.265] iteration 26 : loss : 0.677476, loss_ce: 0.362202
[15:45:48.666] iteration 27 : loss : 0.666758, loss_ce: 0.339082
[15:45:49.054] iteration 28 : loss : 0.625771, loss_ce: 0.244977
[15:45:49.441] iteration 29 : loss : 0.556369, loss_ce: 0.071938
[15:45:49.852] iteration 30 : loss : 0.605680, loss_ce: 0.206851
[15:45:50.237] iteration 31 : loss : 0.662200, loss_ce: 0.356865
[15:45:50.629] iteration 32 : loss : 0.669444, loss_ce: 0.387059
[15:45:51.026] iteration 33 : loss : 0.666017, loss_ce: 0.387165
[15:45:51.416] iteration 34 : loss : 0.610924, loss_ce: 0.241789
[15:45:51.800] iteration 35 : loss : 0.644022, loss_ce: 0.340263
[15:45:52.185] iteration 36 : loss : 0.652779, loss_ce: 0.378835
[15:45:52.571] iteration 37 : loss : 0.614359, loss_ce: 0.269248
[15:45:52.954] iteration 38 : loss : 0.594840, loss_ce: 0.206770
[15:45:53.340] iteration 39 : loss : 0.611246, loss_ce: 0.263719
[15:45:53.755] iteration 40 : loss : 0.589321, loss_ce: 0.188885
[15:45:54.180] iteration 41 : loss : 0.644741, loss_ce: 0.326944
[15:45:54.574] iteration 42 : loss : 0.579535, loss_ce: 0.176354
[15:45:55.011] iteration 43 : loss : 0.565821, loss_ce: 0.135160
[15:45:55.401] iteration 44 : loss : 0.587567, loss_ce: 0.205930
[15:45:55.786] iteration 45 : loss : 0.578485, loss_ce: 0.176621
[15:45:56.169] iteration 46 : loss : 0.593692, loss_ce: 0.233703
[15:45:56.555] iteration 47 : loss : 0.573735, loss_ce: 0.158971
[15:45:56.951] iteration 48 : loss : 0.572851, loss_ce: 0.174162
[15:45:57.336] iteration 49 : loss : 0.580473, loss_ce: 0.211616
[15:45:57.723] iteration 50 : loss : 0.550754, loss_ce: 0.152480
[15:45:58.111] iteration 51 : loss : 0.593763, loss_ce: 0.223032
[15:45:58.497] iteration 52 : loss : 0.617837, loss_ce: 0.304516
[15:45:58.885] iteration 53 : loss : 0.626113, loss_ce: 0.335569
[15:45:59.267] iteration 54 : loss : 0.568378, loss_ce: 0.196423
[15:45:59.649] iteration 55 : loss : 0.608395, loss_ce: 0.284445
[15:46:00.031] iteration 56 : loss : 0.562774, loss_ce: 0.163350
[15:46:00.408] iteration 57 : loss : 0.559524, loss_ce: 0.118123
[15:46:00.799] iteration 58 : loss : 0.554581, loss_ce: 0.104295
[15:46:01.186] iteration 59 : loss : 0.598888, loss_ce: 0.219143
[15:46:01.571] iteration 60 : loss : 0.596721, loss_ce: 0.261095
[15:46:01.995] iteration 61 : loss : 0.570022, loss_ce: 0.203481
[15:46:02.385] iteration 62 : loss : 0.568134, loss_ce: 0.175631
[15:46:02.778] iteration 63 : loss : 0.583645, loss_ce: 0.136306
[15:46:03.163] iteration 64 : loss : 0.580117, loss_ce: 0.192016
[15:46:03.542] iteration 65 : loss : 0.561064, loss_ce: 0.125824
[15:46:03.925] iteration 66 : loss : 0.596141, loss_ce: 0.238544
[15:46:04.307] iteration 67 : loss : 0.565496, loss_ce: 0.200021
[15:46:04.691] iteration 68 : loss : 0.554833, loss_ce: 0.167963
[15:46:05.072] iteration 69 : loss : 0.545982, loss_ce: 0.098956
[15:46:05.459] iteration 70 : loss : 0.532784, loss_ce: 0.091637
[15:46:05.856] iteration 71 : loss : 0.557264, loss_ce: 0.128481
[15:46:06.255] iteration 72 : loss : 0.575746, loss_ce: 0.205888
[15:46:06.649] iteration 73 : loss : 0.562596, loss_ce: 0.160634
[15:46:07.055] iteration 74 : loss : 0.576939, loss_ce: 0.157077
[15:46:07.441] iteration 75 : loss : 0.562286, loss_ce: 0.192204
[15:46:07.824] iteration 76 : loss : 0.578789, loss_ce: 0.140075
[15:46:08.203] iteration 77 : loss : 0.558630, loss_ce: 0.180928
[15:46:08.587] iteration 78 : loss : 0.567997, loss_ce: 0.195400
[15:46:08.959] iteration 79 : loss : 0.552345, loss_ce: 0.144063
[15:46:09.342] iteration 80 : loss : 0.581620, loss_ce: 0.237913
[15:46:09.759] iteration 81 : loss : 0.574325, loss_ce: 0.232319
[15:46:10.154] iteration 82 : loss : 0.577690, loss_ce: 0.239852
[15:46:10.567] iteration 83 : loss : 0.570807, loss_ce: 0.142757
[15:46:11.003] iteration 84 : loss : 0.562426, loss_ce: 0.124162
[15:46:11.431] iteration 85 : loss : 0.538324, loss_ce: 0.111004
[15:46:11.821] iteration 86 : loss : 0.563234, loss_ce: 0.189314
[15:46:12.214] iteration 87 : loss : 0.568552, loss_ce: 0.164100
[15:46:12.598] iteration 88 : loss : 0.539897, loss_ce: 0.142934
[15:46:12.995] iteration 89 : loss : 0.560728, loss_ce: 0.138358
[15:46:13.376] iteration 90 : loss : 0.620231, loss_ce: 0.338845
[15:46:13.761] iteration 91 : loss : 0.581052, loss_ce: 0.239674
